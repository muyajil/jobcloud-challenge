{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "- I tried to work with more complex models, however with the limited time the results were not really useful\n",
    "- The goal from such an experiment should be in my opinion something that is useful and can generate some more insights for future improvement.\n",
    "- Therefore I did not put the focus on the quality of the model (this would take too much time) but on delivering something that can be used quickly\n",
    "\n",
    "## General Idea\n",
    "\n",
    "- There are several ways to utilize this dataset to help people write job titles\n",
    "- I propose some kind of autocomplete feature.\n",
    "- That means the user will type part of a word and receive a list of possible words to autocomplete the current word.\n",
    "- This is a simpler but more realistic approach than recommending complete job titles, since as evident by the data, the job titles are written rather subjective, sometimes with a motivational intro. \n",
    "- Recommending single words is a less complex problem, and also might be a precursor to more sophisticated recommendation system.\n",
    "- Also using a character based model that predicts the next character might be an interesting model, however models like this can predict gibberish, depending on the number of datapoints. This is of course not desired for a user facing feature. Therefore we fall back to recommending single words from a known corpus.\n",
    "- In cases like these it often makes sense simplify the problem as much as possible, so that we can establish a baseline for such a feature and later continue improving it.\n",
    "- Essentially this is a classification problem where the possible words are the target classes.\n",
    "- The input to the model is the input of the user.\n",
    "- We will start by a very simple lookup based model, in the future this can be improved further. There are several reasons for this:\n",
    "    - There seems to be a limited vocabulary for job titles, we can leverage that\n",
    "    - Training data is limited, that means optimizing complex models might be difficult\n",
    "    - It makes more sense to use a very simple model and build the whole application, which can be tested by users.\n",
    "    - Using this approach we can gather feedback rather quickly and optimize the model step by step\n",
    "    - Since the dataset is limited the performance of the application should be able to handle a simple lookup based approach\n",
    "    - We cannot predict words which do not appear in the corpus of existing job titles, however achieving this with a character based model (without predicting gibberish) is really difficult\n",
    "\n",
    "## Preprocess Data\n",
    "\n",
    "- To be able to build such a system we want to extract the single words from the job titles\n",
    "- After that we will do the following preprocessing steps:\n",
    "    - Lowercase all words to reduce the number of target classes\n",
    "    - Remove special characters\n",
    "    - Remove \"m/w\" gender specification (special case, maybe it makes sense to keep it, can be evaluated in a second step)\n",
    "    - Remove workloads from the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5051"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles = pd.read_csv('jobcloud_published_job_titles.csv', header=None)[0]\n",
    "# Lowercase job titles\n",
    "job_titles = job_titles.str.lower()\n",
    "# Remove special characters and numbers (except for umlauts and french accents)\n",
    "job_titles = job_titles.str.replace(\"[^A-Za-z \\u00e4\\u00f6\\u00fc\\u00e1\\u00e8\\u00e9]\", ' ')\n",
    "# Split titles into words\n",
    "job_titles = job_titles.str.split(' ')\n",
    "# Remove lone spaces/empty strings\n",
    "job_titles = job_titles.map(lambda x: list(filter(lambda y: y != ' ' and y != '', x)))\n",
    "# Remove gender specification (obsolete with next step, but better for visibility)\n",
    "job_titles = job_titles.map(lambda x: list(filter(lambda y: y != 'mw', x)))\n",
    "# Remove short words (limited usefulness in recommending stopwords and such)\n",
    "job_titles = job_titles.map(lambda x: list(filter(lambda y: len(y) > 3, x)))\n",
    "# Join everything into one series\n",
    "job_titles = pd.Series([x for y in job_titles for x in y])\n",
    "len(job_titles.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Statistics\n",
    "\n",
    "- To make some useful recommendations we simply recommend the words that appear most first\n",
    "- Therefore we compute the counts of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aarau</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aargau</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacus</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abap</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abend</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "Label        \n",
       "aarau       5\n",
       "aargau      9\n",
       "abacus      3\n",
       "abap        2\n",
       "abend       2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame(job_titles, columns=['Label'])\n",
    "labels['count'] = 1\n",
    "labels = labels.groupby('Label').count()\n",
    "labels.to_json('counts_by_label.json', orient='index')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "- Now we can build the dataset.\n",
    "- For this we need to think about the problem a bit more.\n",
    "- What we are actually doing is a multilabel classification, since many words do have common subwords. For example if the user types \"hel\" the possible labels are \"help\", \"hell\", \"hello\" etc.\n",
    "- However the more characters the user types the less possible labels there are.\n",
    "- Now to build this dataset we start map each subword (starting at the beginning) to the possible target words. \n",
    "- So for a word abcdef we produce the following examples to train the model:\n",
    "    - a, abcdef\n",
    "    - ab, abcdef\n",
    "    - abc, abcdef\n",
    "    - abcd, abcdef\n",
    "    - abcde, abcdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_and_label_generator(job_titles):\n",
    "    unique_words = job_titles.unique()\n",
    "\n",
    "    for word in unique_words:\n",
    "        for i in range(len(word)):\n",
    "            yield word[:i+1], word\n",
    "\n",
    "gen = feature_and_label_generator(job_titles)\n",
    "\n",
    "dataset_file = open('dataset.csv', 'w', encoding='utf-8')\n",
    "dataset_file.write('Input,Label\\n')\n",
    "\n",
    "for feature, label in gen:\n",
    "    dataset_file.write(f\"{feature},{label}\\n\")\n",
    "\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 55079\n",
      "Unique Labels: 5051\n",
      "Unique Inputs: 27797\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print('Inputs:', len(df))\n",
    "print('Unique Labels:', len(df['Label'].unique()))\n",
    "print('Unique Inputs:', len(df['Input'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned above this is a multilabel classification, therefore we must first group the labels by the inputs, so that we know all possible labels for each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>[account, automatiker, automaticien, allrounde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>[aarau, aargau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aar</td>\n",
       "      <td>[aarau, aargau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aara</td>\n",
       "      <td>[aarau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aarau</td>\n",
       "      <td>[aarau]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input                                              Label\n",
       "0      a  [account, automatiker, automaticien, allrounde...\n",
       "1     aa                                    [aarau, aargau]\n",
       "2    aar                                    [aarau, aargau]\n",
       "3   aara                                            [aarau]\n",
       "4  aarau                                            [aarau]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby('Input').agg({'Label':lambda x: list(x)}).reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now for each input we can sort the outputs based on the counts we computed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>[assistant, analyst, assistent, aussendienst, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>[aargau, aarau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aar</td>\n",
       "      <td>[aargau, aarau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aara</td>\n",
       "      <td>[aarau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aarau</td>\n",
       "      <td>[aarau]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Input                                              Label\n",
       "0      a  [assistant, analyst, assistent, aussendienst, ...\n",
       "1     aa                                    [aargau, aarau]\n",
       "2    aar                                    [aargau, aarau]\n",
       "3   aara                                            [aarau]\n",
       "4  aarau                                            [aarau]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_by_label = json.load(open('counts_by_label.json'))\n",
    "\n",
    "df['Label'] = df['Label'].map(lambda x: sorted(x, reverse=True, key=lambda y: counts_by_label[y]['count']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce output\n",
    "\n",
    "- Now we can output this lookup into a simple dictionary which the application can load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Input')\n",
    "df.to_json('labels_by_input.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('jobname_app/labels_by_input.json', orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobcloud_challenge",
   "language": "python",
   "name": "jobcloud_challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
